---
title: "R Notebook"
output: html_notebook
---

# R for Excel Users

Like most people, I first learned to work with numbers through an Excel spreadsheet. After graduating with an undergraduate philosophy degree, I somehow convinced a medical device marketing firm to pay me to write reports on the bone graft market. When I first started I remember not knowing how to copy and paste values between cells, but after a few years at various jobs I became fairly proficient with the tool, and was able to build all sorts of useful models. When you think about it, this is an amazing feature of Excel. Every day, all over the world people open up a spreadsheet to do some data entry and then, bit by bit, learn to do increasingly complex analytical tasks. Excel is a master at teaching people how to use excel. 

R is not like that. I learned to use R as a side project during law school and it felt a bit like learning kung fu from an abusive ascetic in the mountains of rural china. 

![](pai_mei.gif)

I couldn't get R to do anything. I couldn't read in files, I couldn't draw a plot or multiply two numbers together. All I could do was generate mystifying errors and get mocked on Stack Overflow for asking redundant questions. All this was made more frustrating by the fact that I could easily accomplish all of these things in Excel without experiencing any pain. 

This is the basic pain of learning to program. Programming languages are designed to be general in their application and to allow you to accomplish a huge variety of complex tasks with the same set of tools. The cost of this generality is a steep learning curve. When you start learning to do basic tasks in R, you are also learning the tools which will allow you to accomplish very complex tasks down the road. What this means is that as you learn more and more the marginal cost of more complex analyses goes down. Excel is easy at the beginning, difficult at the end, while R is the opposite. If you were to graph this it might look like this: 

```{r, echo=FALSE}
library(tidyverse)
df <- data_frame(
  Complexity = 1:1000,
  R = (Complexity *2) ^ .5,
  Excel = (Complexity / 100) ^ 2
)

df %>% 
  gather(tool, value, -Complexity) %>% 
  ggplot(aes(x = Complexity, y = value, group = tool, colour = tool)) +
  geom_line() +
  ylab("Difficulty") +
  ggtitle("Difficulty vs. Complexity")

```

At the beginning, when you are trying to accomplish simple things like balanacing a budget or entering some data by hand, R is definitely harder to learn. However, as the task gets more and more complex, it becomes easier learn to accomplish it using R than Excel. The reason behind this is the core structures of Excel are designed for relatively simple use cases, which are not the best for more complex scenarios. This isn't to say that you can't solve a lot of complex problems with Excel, it's just that the tool won't make it easy for you. 

For a lot of us, the pain of learning to program feels like the pain of failure. When the program gives us an incromprehensible error message it feels like its telling us that we're stupid and can't do it. But what you learn after programming for a while, and working with other professional developers is that nobody really understands those errors, and everybody feels like an imposter when their program fails. The pain you feel is not the pain of failure, it's the pain of learning. 

### Why is learning new things so hard?!

The difficulty of learning a new tool is caused by two obstacles: 

Obstacle #1) The tool is different from what you know

When you know how to use something you have this vast amount of basic vocabulary about that tool. Even after not using Excel for four years, I can still remember all the hotkeys, formula names, and menue structure from the program. You might know where to look to find help, or how to google questions in such a way that you find useful answers. When you're learning a new tool, you lack all of that knowledge, which is painful. 

Obstacle #2) The mental model underlying the tool is different from your mental model

This means that the way the tool wants you to think about the problem is different from the way you are used to thinking about the problem. For instance, if you are used to putting your analysis in a rectangular grid, then moving to a tool which is designed around procedural commands is going to be difficult. 

In my opinion obstacle #2 this is by far the larger barrier for Excel users. Most of the people who learn R have some basis in programming. The mental models underlying languages like Matlab or Python as well as statistical packages like SPSS and SAS have a lot in common with R, and there are a lot of resources available for translating the bits which don't make sense. 

### Four Fundamental Differences

####1) Text-based analysis

Excel is based on the physical spreadsheet, or accountant's ledger. This was a large piece of paper with rows and collumns. Records were stored in the first column on the left, calculations on those records were stored in the boxes to the right, and the sum of those calculations was totaled at the bottom. I would call this a referential model of computation which has a few qualities: 
- The data and computation are usually stored in the same place
- Data is identified by its location on the grid. Usually you don't name a data range in excel, but instead refer to it by its location for instance with `$A1:C$36`
- The calculations are usually the same shape as the data. In other words if you have a list of 20 numbers which you want to multiply by 2 you have a column of 20 rows with `=A1 * 2, =A2 * 2, ...., An * 2`. 


Text based data analyis is different: 
- Data and computation are separate. You have one file which is the data and another file which is the script which tells the program how to manipulate that data. This leads to a procedural kind of model in which the raw data is fed through a set of instructions and the output pops out the other side. 
- Data is generally referenced by name. Instead of having a dataset which lives in the range of `$A1:C$36` you name the dataset when you read it in, and refer to it by that neme whenever you want to do something with it.


####2) Data structures

Excel has only one basic data structure: the cell. Cells are extremely flexible in that they can store numeric, character, logical or formulas information and any cell can have any of these objects. You can refer to ranges of cells together, but the basic data structure is still the cell. 

The basic R data structure is a vector. You can think of a vector like a column in an Excel spreadsheet with the limitation that all the data in that vector much be of the same type. If it is a character vector, then every element must be a character, if it is a logical vector then every element must be `TRUE` or `FALSE`. There's no such constraint in Excel, you might have a collumn which has a bunch of numbers, but then some explanatory test intermingled with the numbers. This isn't allowed in R. 

####3) Iteration

Iteration is one of the most powerful features of programming languages and is a big adjustment for Excel users. All iteration is is just getting the computer to do the same thing over and over again for some period of time. Maybe you want to draw the same graph based on fifty different data sets, or read and filter a lot of data tables. In a programming language like R you write a script which works for all of the cases you want to apply it to, and then tell the computer to do the application. 

Excel analysts typically do a lot of this iteration themselves, and kind of take the place of the iteration parts of most R scripts. For instance if you wanted to combine ten different xls files into one big file you will probably open each one indiivdually, copy the data, and paste it into a master spreadsheet. In effect the analyst is taking the place of a `for` loop by doing one thing over and over again until a condition is met. 


####4) Simplification through abstraction

Another major difference is that programming encourages you to simplify your analysis by abstracting common functions from that analysis. In the example above you might find that you have to read in the same type of files over and over again and check that they have the right number of rows. R allows you to write a function which does this: 

``` {r }
read_and_check <- function(file){
  out <- read.csv(file)
  if(nrow(out) == 0) {
    stop("There's no data in this file!")
  } else {
    out
  }
}
```

All this file does is read in a `.csv` file and then check to see if it has any rows, if it doesn't it returns an error, otherwise it returns the file (which is called "out"). This is a powerful approach because it helps you save time and reduce errors. 

There's really no analog in Excel-based workflow, and when most analysts get to this point they just start writing VBA code to do some of this work. 


### Example: Joining two tables together

I thought I'd illustate these principles by working through the example of joining two tables together in Excel and R. Let's say that we had two data tables, one with some information about cars and another with the colour of those cars, and we want to join the two of them together. For the purpose of this exercise we're going to assume that the number of cylendars in a car determines it's colour. 

```{r }
library(dplyr)
library(knitr)
cars <- mtcars
colours <- data_frame(
  cyl = unique(cars$cyl),
  colour = c("Blue", "Green", "Eggplant")
)

kable(cars)
```
```{r }
kable(colours)

```

In Excel you would probably do this using the `VLOOKUP()` function, which takes a key, and a range and looks up the value of that key within that range. I put together an example spreadsheet of this approach (here)[https://docs.google.com/spreadsheets/d/1K2IqdXX2MoUB4BorRcBcruS7spCvRtDwqXV-4gYAob4/edit?usp=sharing]. Notice that in each cell that we want to lookup we typed some version of `=vlookup(C4,$H$2:$I$5, 2, FALSE)` This illustrates a few things. First the calculation is the same shape as the data, and happens in the same file as the data. We have as many formulas as we have things that we want to lookup, and they are placed right next to the dataset. Second the data is refered to by its address on the sheet. If we move the lookup table to another sheet, or another place on this sheet that is going to screw up out lookup. Third, notice that the first entry of the `cyl` column in the spreadsheet store in `C2` is stored as text, while the rest are all nuemric which causes an error in the lookup function. 

To do the same thing in R we would use this code: 

```{r }

left_join(cars, colours, by = "cyl") %>% 
  kable() #Kable just displays the table in a nice way

```

Here we refer to the data by its name, use one function to operate on the whole table rather than row by row, and because consistency is enforced for each vector there's no character in the first place. 


### Iteration

Now let's say we wanted to get the mean displacement for each colour of car. Most Excel users would probably do this iteration mannually, first selecting the table and sorting it by colour and then picking out the ranges that they wanted to average. A more sophisticated analyst would probably use the `averageif()` function to pick out the criteria they wanted to average on, and so avoid a few errors. Both approaches are implemented in the `iteration` tab of the spreadsheet. 

In R you would do something like this: 

```{r}

left_join(cars, colours, by = "cyl") %>% 
  group_by(colour) %>% 
  summarize(mean_displacement = mean(disp)) %>% 
  kable()

```

What this does is takes the data set, splits it up by the grouping variable, in this case colour, then applies the function in the `summarize` function to calculate the grouped summary statistic. Again, the difference is that we're always refering to things by name rather than location, there is one line of code which applies to each group, and all of the actions are stored in the script. 

### Generalizing through functions
Functions are among the more difficult parts of learning to program, and you really can get by for quite a long time without ever learning to use them. I wanted to include them just because they are commonly used and can be quite discouraging for Excel users because they are totally foreign to the excel workflow. A function is a way of using existing code on new objects. In the case above it might look like this:


``` {r}

join_and_summarize <- function(df, colour_df){
  left_join(df, colour_df, by = "cyl") %>% 
    group_by(colour) %>% 
    summarize(mean_displacement = mean(disp))
}

```

The things between the `function()` braces are called "arguments", and when you call the function all it does is take the actual objects you supply to the funciton and plugs them in to wherever that argument appears between the curly braces. In this case we would supply `cars` in the `df` argument, and it would just replace all the `df`'s it can find with the cars object. 

```{r}
join_and_summarize(cars, colours) %>% 
  kable() #again just for presentation
```

### Conclusion
Excel users have a strong mental model of how data analysis works, and in some ways it is more difficult to learn a new model when you already have one that works for a lot of cases. However, learning to program will allow you to do things that you can't do easily in Excel, and it really is worth the pain of learning the new model. 














