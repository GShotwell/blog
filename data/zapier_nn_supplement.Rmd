---
title: "Neural Network Classifier"
author: "Gordon Shotwell"
date: '2018-01-30'
output: html_document
---

My first attempt at this problem ended in a bit of a failure. I tried to train a set of individual logistic regression algorithms on the outcome variables, but unfortunately several of those models failed to converge. I suspected that a neural network would do a better job on this problem for two main reasons. 

First, neural networks can naturally output multi-label classifiers. I had to do a lot of gymnastics to try to get a set of binary classifiers to output overlapping labels, but all you have to do to get multi-label classification out of a neural network is to add nodes to the output layer.

Second, neural networks are really good at learning from context. For instance, if you want to create a binary classifier which considers the interaction of several words you typically have to create a new variable to include in the model. Neural networks can build up these interactions on their own without requiring the analyst to recode their data ahead of time. 

To start with a network we need to clean the data in the same way that we would for a binary classifier. 

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
library(keras)
train <- read_csv("train.csv")
train$id <- as.factor(train$id)
```

```{r, data cleaning, cache = TRUE}
txt <- train %>% 
    select(id, comment_text) %>% 
    unnest_tokens(word, comment_text, ) %>% 
    anti_join(stop_words, by = "word") 

txt$word <- str_replace_all(txt$word, "[:punct:]", "") %>% 
    str_replace_all("[:digit:]", "")

labels <- train %>% 
    select(id, toxic:identity_hate) %>% 
    gather(label, value, -id) %>% 
    group_by(id) %>% 
    mutate(unlabeled = ifelse(sum(value) == 0, 1, 0)) %>% 
    ungroup() %>% 
    spread(label, value)

txt <- labels %>% 
    left_join(txt, by = "id")

txt$word <- SnowballC::wordStem(txt$word) 

common_words <- txt %>% 
    ungroup() %>% 
    gather(label, value, -id, -word) %>% 
    filter(value == 1) %>% 
    filter(word != "id") %>% 
    filter( nchar(word) > 1 & nchar(word < 40)) %>% 
    group_by(label, word) %>% 
    summarize(n = n()) %>% 
    arrange(desc(n)) %>% 
    top_n(500) %>% 
    pull(word) %>% 
    unique()
```

Next we need to encode this data in a way that the network can understand. I'm going to use a one-hot matrix where each row is an observation, and each column is a word. The entries of the matrix are `1` if the observed string has that word, and `0` otherwise. 

```{r one hot, cache = TRUE}
x <- txt %>% 
    select(id, word) %>% 
    filter(word %in% common_words) %>% 
    group_by(id, word) %>% 
    tally() %>% 
    select(-n) %>% 
    mutate(one_hot = 1) %>%
    spread(word, one_hot, fill = 0)
```

We can create our test and training sets using this one-hot matrix, and the matrix of labels which is already in the right form. The output matrix has the same number of rows as our observation, and one column per possible label. 

```{r test and train, cache = TRUE}
test_idx <- sample(c(TRUE, FALSE), nrow(x), replace = TRUE, c(.7, .3))

x_train <- x[test_idx, ] %>% 
    ungroup() %>% 
    select(-id) %>% 
    as.matrix()
y_train <- labels %>% 
    filter(id %in% x$id) %>% 
    select(-id, -unlabeled) %>% 
    .[test_idx,] %>% 
    as.matrix()

x_test <- x[!test_idx, ] %>% 
    ungroup() %>% 
    select(-id) %>% 
    as.matrix()
y_test <- labels %>% 
    filter(id %in% x$id) %>% 
    select(-id, -unlabeled) %>% 
    .[!test_idx,] %>% 
    as.matrix()
```

Now we're ready to construct the network. I'm using the wonderful new Keras package from RStudio, and mostly default values for the training parameters. 

```{r neural network, message=FALSE, warning=FALSE, cache = TRUE}

tox_mod <- keras_model_sequential() %>% 
    layer_dense(units = 16, activation = "relu", input_shape = ncol(x_train)) %>% 
    layer_dropout(.3) %>% 
    layer_dense(units = 16, activation = "relu") %>%
    layer_dropout(.2) %>% 
    layer_dense(units = 16, activation = "relu") %>% 
    layer_dense(units = ncol(y_train), activation = "sigmoid")

val_indices <- sample(1:nrow(x_train), 20000)

x_val <- x_train[val_indices,]
partial_x_train <- x_train[-val_indices,]

y_val <- y_train[val_indices,]
partial_y_train <- y_train[-val_indices,]

tox_mod %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history <- tox_mod %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)
plot(history)
```

This plot shows that the model is getting better each epoch and doesn't appear to be overfitting. It also more or less plateaus after about the fifth epoch, so we could cut off training there if we were worried about computation resources. 

Finally we can output some predictions to check the model: 

```{r prediction, cache = TRUE}
preds <- tox_mod %>% 
    predict(x_test) %>% 
    as.data.frame()
y_test <- as.data.frame(y_test)
map2_dbl(preds, y_test, ~MLmetrics::LogLoss(.x,.y)) %>% 
    mean()
```

The leader board on the Kaggle competition tells me that the best mean log-loss score is `0.052` and while that's quite a bit lower than my score the fact that it's in the same ballpark is encouraging. 


